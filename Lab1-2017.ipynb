{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de aprendizaje automático 2017 laboratorio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajaremos con un conjunto de datos de textos. Para ello los descargamos.\n",
    "Una descripción del set de datos: http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', categories=['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med'], shuffle=True, random_state=42)\n",
    "test = fetch_20newsgroups(subset='test', categories=['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar un ejemplo de entrenamiento de cada una de las categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "************************************talk.politics.guns*****************************\n",
      "************************************************************************\n",
      "From: tms@cs.umd.edu (Tom Swiss (not Swift, not Suiss, Swiss!))\n",
      "Subject: Re: Clinton wants National ID card, aka USSR-style \"Internal Passport\"\n",
      "Organization: The Reality Liberation Front (pixels to the people!)\n",
      "Lines: 17\n",
      "\n",
      "slp9k@cc.usu.edu writes:\n",
      ">\n",
      ">\tI just want to point out that while I am fully in support of privacy,\n",
      ">it will be possible soon to have a completely secure ID card, useable in bank\n",
      ">transactions, medical, etc etc.\n",
      "\n",
      "     There is no such thing as \"completely secure,\" especially when dealing\n",
      "with High Technology. It's all a question of cost: what cost are you\n",
      "willing to bear to protect your information vs. what rewards the \"bad guys\"\n",
      "are going to get if they break it. The rewards of breaking such a single ID\n",
      "system would be high indeed.\n",
      "\n",
      "===============================================================================\n",
      "Tom Swiss/tms@cs.umd.edu  |  \"Born to die\"   |   Keep your laws off my brain!\n",
      "     \"What's so funny 'bout peace, love and understanding?\" - Nick Lowe \n",
      "     This .sig contains no animal products and was not tested on animals.\n",
      "  \"Time is just nature's way of keeping everything from happening at once.\"\n",
      "\n",
      "************************************************************************\n",
      "************************************soc.religion.christian*****************************\n",
      "************************************************************************\n",
      "From: Petch@gvg47.gvg.tek.com (Chuck Petch)\n",
      "Subject: Daily Verse\n",
      "Organization: Grass Valley Group, Grass Valley, CA\n",
      "Lines: 4\n",
      "\n",
      "For whoever does the will of my Father in heaven is my brother and sister\n",
      "and mother.\" \n",
      "\n",
      "Matthew 12:50\n",
      "\n",
      "************************************************************************\n",
      "************************************sci.med*****************************\n",
      "************************************************************************\n",
      "From: annick@cortex.physiol.su.oz.au (Annick Ansselin)\n",
      "Subject: Re: Is MSG sensitivity superstition?\n",
      "Nntp-Posting-Host: cortex.physiol.su.oz.au\n",
      "Organization: Department of Physiology, University of Sydney, NSW, Australia\n",
      "Lines: 29\n",
      "\n",
      "In <C5nFDG.8En@sdf.lonestar.org> marco@sdf.lonestar.org (Steve Giammarco) writes:\n",
      "\n",
      ">>\n",
      ">>And to add further fuel to the flame war, I read about 20 years ago that\n",
      ">>the \"natural\" MSG - extracted from the sources you mention above - does not\n",
      ">>cause the reported aftereffects; it's only that nasty \"artificial\" MSG -\n",
      ">>extracted from coal tar or whatever - that causes Chinese Restaurant\n",
      ">>Syndrome.  I find this pretty hard to believe; has anyone else heard it?\n",
      "\n",
      "MSG is mono sodium glutamate, a fairly straight forward compound. If it is\n",
      "pure, the source should not be a problem. Your comment suggests that \n",
      "impurities may be the cause.\n",
      "My experience of MSG effects (as part of a double blind study) was that the\n",
      "pure stuff caused me some rather severe effects.\n",
      "\n",
      ">I was under the (possibly incorrect) assumption that most of the MSG on\n",
      ">our foods was made from processing sugar beets. Is this not true? Are \n",
      ">there other sources of MSG?\n",
      "\n",
      "Soya bean, fermented cheeses, mushrooms all contain MSG. \n",
      "\n",
      ">I am one of those folx who react, sometimes strongly, to MSG. However,\n",
      ">I also react strongly to sodium chloride (table salt) in excess. Each\n",
      ">causes different symptoms except for the common one of rapid heartbeat\n",
      ">and an uncomfortable feeling of pressure in my chest, upper left quadrant.\n",
      "\n",
      "The symptoms I had were numbness of jaw muscles in the first instance\n",
      "followed by the arms then the legs, headache, lethargy and unable to keep\n",
      "awake. I think it may well affect people differently.\n",
      "\n",
      "************************************************************************\n",
      "************************************comp.graphics*****************************\n",
      "************************************************************************\n",
      "Subject: newss\n",
      "From: pollarda@physc1.byu.edu\n",
      "Distribution: world\n",
      "Organization: Brigham Young University\n",
      "Lines: 24\n",
      "\n",
      "\n",
      "\n",
      "I am working on a project where we are going to be including\n",
      "both still and moving grapics within a database.  Of course\n",
      "JPEG and MPEG come to mind as the formats of choice for the\n",
      "various files.  However, from what I read on the Net, it seems\n",
      "as if there are several different forms of each of these.\n",
      "\n",
      "What I want to do, is settle on a file format which I can count\n",
      "on as being a standard format 10 years from now.  I know Apple is going\n",
      "to support Quicktime on the new Power PC's and, so this\n",
      "may be the format of choice.\n",
      "\n",
      "What format does Apple's Quicktime use for their products?  I guess\n",
      "it is some kind of MPEG for their motion picture. Is it any different\n",
      "than standard MPEG files?\n",
      "\n",
      "Thanx for any info!\n",
      "\n",
      "Art.\n",
      "Pollarda@xray.byu.edu\n",
      "\n",
      "\n",
      "  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories=set(['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med'])\n",
    "cont = 0\n",
    "while len(categories) != 0 and cont < len(train.data):\n",
    "    if train.target_names[train.target[cont]] in categories:\n",
    "        print('************************************************************************')\n",
    "        print('************************************'+train.target_names[train.target[cont]]+'*****************************')\n",
    "        print('************************************************************************')\n",
    "        print train.data[cont]\n",
    "        categories.remove(train.target_names[train.target[cont]])\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar el módulo CountVectorizer para extraer caracterísitcas a partir de texto y explicar cómo funciona. Explicar qué es el modelo de bolsa de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El módulo CountVectorizer consiste en varias etapas de procesamiento:\n",
    "\n",
    "Tokenización:Tokeniza los documentos separandolos por espacios en blanco y símbolos de puntuación. A cada diferente token le asigna un id de tipo entero. El patrón para delimitar los tokens se puede pasar por parámetro. \n",
    "Conteo:Cuenta las ocurrencias de los tokens en cada documento.\n",
    "Normalización:normaliza las ocurrencias y penaliza a las palabras que ocurren en la mayoría de los documentos.\n",
    "\n",
    "Esto luego se lleva a una matriz donde cada fila de la matriz es un documento y cada columna es un token como se explica abajo en la Bolsa de palabras. A este proceso se le llama vectorizar, ya que dado un corpus, lo traduce a un conjunto de características de vectores. \n",
    "\n",
    "Este módulo contiene varias opciones, entre las más destacadas se encuentran:\n",
    "-N gramas: los n-gramas son secuencias de palabras consecutivas. Por ejemplo los bigramas contienen secuencias de dos palabras consecutivas, mientras que los trigramas contienen tres palabras.\n",
    "-Tokenizador propio: se puede utilizar cualquier tokenizador basado en expresiones regulares.\n",
    "-Stop words: acepta una lista de palabras que se ignoran para el vector de características. Generalmente estas palabras se ignoran porque se repiten en casi todos los documentos, por lo que no aportan.\n",
    "-Vocabulario: si se tiene conocimiento sobre el dominio del problema se le puede pasar ya un vocabulario previamente creado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La bolsa de palabras es un modelo para representar un documento o texto, el cual se puede pensar como un conjunto de palabras el cual mantiene la cantidad de ocurrencias de las mismas. Como es un conjunto, esto significa que se pierde el orden de las palabras, lo que termina ocasionando la perdida de la gramática. Es una representación que a priori parece que pierde mucha información, pero en la práctica da muy buenos resultados.\n",
    "\n",
    "Su representación computacional, puede ser la de una matriz m[i,j] donde i representa un documento y j representa la palabra j y el valor [i,j] representa la cantidad de ocurrencias de la palabra j en el documento i. \n",
    "Debido a que un documento generalmente contiene muchas menos palabras que las que se consideran, esta representación de matriz es muy dispersa, por lo que en la práctica se suele utilizar otro tipo de estructuras que aprovechan mejor los recursos.\n",
    "\n",
    "En función del conocimiento que se tenga sobre la realidad del problema a resolver, se plantean dos grandes formas de modelar mediante bolsa de palabras un documento. Si el conocimiento es mucho y se incurrió en un análisis previo, se puede modelar cada documento con ciertas palabras ya fijadas. En el caso que no haya mucho análisis de la realidad, se pueden utilizar todas las palabras que aparezcan en los documentos. Como regla general, se pueden omitir la lista de palabras denominadas 'stop words' las cuales generalmente no aportan en las decisiones. Dichas 'stop words' pueden ser aprendidas implicitamente con un modelo tf-idf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar un clasificador bayesiano sencillo Multinomial. Explicar cada uno de los parámetros de este clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, train.target)\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "predicted = clf.predict(X_new_counts)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador bayesiano Multinomial tiene 3 parámetros.\n",
    "\n",
    "Alfa: valor numérico. Este parámetro sirve para el suavizado cuando se aplica máxima verosimilitud para calcular la probabilidad de que alguna caracterísitca (en este caso la palabra) ocurra en determinada clase. El valor por defecto es 1.0, también conocido como el suavizado de Laplace.\n",
    "\n",
    "fit_prior : booleano. Determina si aprende las probabilidades a priori de las clases a partir de los datos de entrenamiento o si le asigna una probabilidad uniforme a cada clase (ML).\n",
    "\n",
    "class_prior : arreglo. Arreglo de probabilidades a priori de las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetir el proceso pero utilizando el módulo TfidfTransformer en lugar de CountVectorizer. Explicar la medida Tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train.target)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchos textos, hay palabras que se repiten como artículos, conjunciones, algunos adverbios, pronombres, etc. En general, este tipo de palabra aporta poca información a la hora de clasificar los documentos o en la tarea de recuperación de información. Debido a esto es que se propone el modelo tf-idf, el cual juega con la frecuencia de las palabras en el documento y a su vez penaliza a las palabras que aparecen en muchos documentos.\n",
    "\n",
    "Con respecto a la frecuencia de las palabras, hay varios modelos, como por ejemplo el de conteo de las palabras como ya vimos en bolsa de palabras. Un modelo muy utilizado es el de frecuencia de logaritmo, el cual le agrega un valor mas alto de frecuencia a las palabras que aparecen mas en el documento, pero dicho aumento no es lineal con la cantidad de ocurrencias, sino que es logaritmico. Otro posible modelo puede tener en cuenta la cantidad de palabras en el documento para determinar su frecuencia.\n",
    "\n",
    "La frecuencia inversa del documento es una medida de cuanta información provee la palabra, es decir si el término es común o es raro a través de los documentos. La idea detrás es que los términos que aparecen pocas veces aportan mucha más información que los que aparecen mucho en todos los documentos. Para calcular este valor, primero se calcula la cantidad de documentos en los que aparece el término, luego se divide la cantidad total de documentos sobre este valor y finalmente se le aplica el logaritmo al resultado. Ese resultado es la frecuencia inversa del documento para un término.\n",
    "\n",
    "Finalmente cabe destacar que la medida tf-idf se calcula como el producto tf * idf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluya los pasos de extracción de características y el clasificador dentro de un pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso del pipeline es por simplicidad y además porque es más facil a la hora de probar con diferentes valores en los hiper parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluir las métricas de precision, recall y fscore para cada cada una de las clases y la matriz de confusión probando contra el conjunto de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.99      0.81      0.89       396\n",
      "soc.religion.christian       0.80      0.99      0.89       398\n",
      "    talk.politics.guns       0.94      0.96      0.95       364\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1547\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[345,   2,  29,  13],\n",
       "       [  9, 320,  57,  10],\n",
       "       [  2,   1, 395,   0],\n",
       "       [  2,   1,  11, 350]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "pipe.fit(train.data, train.target)\n",
    "docs_test = test.data\n",
    "predicted = pipe.predict(docs_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test.target, predicted, target_names=test.target_names))\n",
    "metrics.confusion_matrix(test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de las métricas se puede ver un muy buen resultado en todas las clases y en todas las medidas de performance. \n",
    "En base a la matriz de confusión, se puede ver que la predicción de la religión es la que se podría mejorar, ya que es la columna que presenta los valores más altos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluya una GridSearch para ver cuales son los mejores hiper-parámetros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vect__stop_words' : ('english', None),\n",
    "              'vect__binary' : (True,False),\n",
    "              'tfidf__norm' : ('l1','l2',None),\n",
    "              'tfidf__use_idf' : (True,False),\n",
    "              'tfidf__smooth_idf' : (True,False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hiper-parámetros del clasificador MultinomialNB fueron explicados anteriormente. \n",
    "En cuanto al CountVectorizer, se eligieron tres parámetros:\n",
    "\n",
    "ngram_range: Considera dos posibilidades, unigrama y bigramas. Los ngramas son un modelo de lenguaje. El caso del unigrama es bien sencillo, simplemente cuenta la cantidad de ocurrencias de la palabra. Mientras que en el caso de los bigramas además cuenta las ocurrencias de a dos palabras juntas.\n",
    "\n",
    "stop_words: Las stop_words como ya se explicó, son palabras del idioma que generalmente no aportan contenido para la clasificación, ya que son palabras que ocurren en casi todos los documentos. Los valores posibles para esta ejecución son de ignorarlas o de considerarlas.\n",
    "\n",
    "binary: El modelo binario es el más simple de todos. No cuenta las ocurrencias de las caracerísticas en el modelo, sino que pone 1 cuando aparece la característica en el documento y 0 cuando no aparece. Basicamente cuenta si aparece la palabra o no en el documento, no cuantas veces aparece.\n",
    "\n",
    "Para el TfidfTransformer, se eligieron tres parámetros:\n",
    "\n",
    "norm: refiere al tipo de función a computar para el cálculo de la frecuencia del término. Puede ser la función logarítmica, la función coseno o simplemente el conteo.\n",
    "\n",
    "use_idf: Si habilitar o deshabilitar idf.\n",
    "\n",
    "smooth_idf: Esto es para el suavizado del idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exlique cada uno de los parámetros de cada uno de los pasos del pipeline y pruebe con varios tipos de clasificadores de los vistos en el curso. ¿Con qué configuración de parámetros obtuvo mejores resultados? Aplique selección de atributos y valore los resultados obtenidos. Realice una prueba final contra el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accracy:\n",
      "Naive Bayes:0.985794231597\n",
      "KNN:0.949203616014\n",
      "ID3:0.854928971158\n"
     ]
    }
   ],
   "source": [
    "nb_clf = GridSearchCV(pipe, parameters, n_jobs=-1).fit(train.data, train.target)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "knn_clf = GridSearchCV(pipe, parameters, n_jobs=-1).fit(train.data, train.target)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "id3_clf = GridSearchCV(pipe, parameters, n_jobs=-1).fit(train.data, train.target)\n",
    "\n",
    "print 'Accuracy:'\n",
    "print 'Naive Bayes:' + str(nb_clf.best_score_)\n",
    "print 'KNN:' + str(knn_clf.best_score_)\n",
    "print 'ID3:' + str(id3_clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "Naive Bayes:0.985794231597\n",
    "KNN:0.949203616014\n",
    "ID3:0.854928971158\n",
    "\n",
    "Considerando la medida Accuracy como medida de performance para evaluar los modelos, se desprende que el mejor clasificador fue Naive Bayes con los siguientes hiper-parámetros del modelo:\n",
    "{'vect__ngram_range': (1, 2), 'tfidf__smooth_idf': True, 'tfidf__norm': None, 'vect__binary': False, 'tfidf__use_idf': True, 'vect__stop_words': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "\n",
    "\n",
    "count_feats = range(100,200000,1000)\n",
    "parameters = {'feat__k' : count_feats}\n",
    "pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                 ('feat', feature_selection.SelectKBest(feature_selection.chi2)),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MultinomialNB()),\n",
    "])\n",
    "nb_feat_select_clf = GridSearchCV(pipe, parameters, n_jobs=-1).fit(train.data, train.target)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964700817908\n",
      "{'feat__k': 10100}\n"
     ]
    }
   ],
   "source": [
    "print nb_feat_select_clf.best_score_\n",
    "print nb_feat_select_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando selección de atributos y con un gap de 1000, vemos que la mejor performance es cuando tiene 10100 atributos. \n",
    "0.964700817908\n",
    "{'feat__k': 10100}\n",
    "Aún así, funciona mejor con la totalidad de los atributos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89851325145442795"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe.fit(train.data, train.target)\n",
    "predicted = pipe.predict(test.data)\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien la accuracy utilizando cross-validation (el por defecto 3) en GridSearch fue de 98.5%, para el conjunto de testeo es de casi un 10% menos. Se podría haber aumentado el k en k-folding a 10 tal vez y haber hecho pruebas con eso, pero por temas de tiempo, se eligió el por defecto. Aún así, la prueba de accuracy utilizando cross-validation es más representativa que probando contra el conjunto de testeo ya que el cross-validation no sufre de 'high-variance estimate' ya que por defecto GridSearch se encarga de crear los los conjuntos k bien distribuidos en cuanto a sus documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repita el ejercicio utilizando HashingVectorizer para extraer características. Explique las ventajas de este método de extracción de caraterísticas frente a CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85972850678733037"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "parameters = {'vect__n_features' : count_feats}\n",
    "pipe = Pipeline([('vect', HashingVectorizer(ngram_range=(1,2), non_negative=True)),\n",
    "                  ('clf', MultinomialNB()),\n",
    "])\n",
    "nb_hashing_clf = GridSearchCV(pipe, parameters, n_jobs=-1).fit(train.data, train.target)  \n",
    "\n",
    "predicted = nb_hashing_clf.predict(test.data)\n",
    "print nb_hashing_clf.best_score_\n",
    "print nb_hashing_clf.best_params_\n",
    "print np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.907447266466\n",
    "{'vect__n_features': 11100}\n",
    "0.86102133161\n",
    "\n",
    "Vemos que utilizando CountVectorizer +TfIdfTransformer anduvo mejor que utilizando HashingVectorizer. El primero tuvo un 8% mas de accuracy sobre el cross validation y un 4% sobre el cuerpo de testeo. A partir de la cantidad de características elegidas, podemos resaltar que esa cantidad es muy parecida a la que devolvió el método SelectKBest.\n",
    "\n",
    "Las principales ventajas están en la forma de almacenamiento. Utiliza una función de Hash para pasar los términos a un índice, por lo que ahorra espacio de memoria al no tener que guardar un diccionario. También viene con un parámetro opcional para elegir la cantidad de features a representar. Por otro lado, la matriz que utiliza es dispersa, por lo que no guarda los 0's en los documentos que no tienen esa característica. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
